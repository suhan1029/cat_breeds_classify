{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader, ConcatDataset\n",
    "from collections import Counter\n",
    "import torch.cuda.amp as amp\n",
    "from torch import nn, optim\n",
    "import copy\n",
    "from torch.optim import lr_scheduler\n",
    "import timm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN의 Generator 정의\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc, ndf, image_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_usage():\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], \n",
    "                            stdout=subprocess.PIPE)\n",
    "    return int(result.stdout.decode('utf-8').strip())\n",
    "\n",
    "class DynamicDataLoader:\n",
    "    def __init__(self, dataset, batch_size=32, num_workers=4, pin_memory=True, prefetch_factor=2):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.prefetch_factor = prefetch_factor\n",
    "        self.loader = self.create_loader()\n",
    "        self.adjusting = False\n",
    "        self.target_gpu_usage = 95  # Target GPU usage in percent\n",
    "\n",
    "    def create_loader(self):\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, \n",
    "                          pin_memory=self.pin_memory, prefetch_factor=self.prefetch_factor, persistent_workers=True)\n",
    "\n",
    "    def adjust_num_workers(self):\n",
    "        while self.adjusting:\n",
    "            gpu_usage = get_gpu_usage()\n",
    "            print(f\"Current GPU usage: {gpu_usage}%\")\n",
    "            if (gpu_usage < self.target_gpu_usage - 10) and (self.num_workers < 16):\n",
    "                self.num_workers += 1\n",
    "                print(f\"Increasing num_workers to {self.num_workers}\")\n",
    "            elif (gpu_usage > self.target_gpu_usage + 10) and (self.num_workers > 1):\n",
    "                self.num_workers -= 1\n",
    "                print(f\"Decreasing num_workers to {self.num_workers}\")\n",
    "            self.loader = self.create_loader()\n",
    "            time.sleep(20)\n",
    "\n",
    "    def start_adjusting(self):\n",
    "        self.adjusting = True\n",
    "        self.adjust_thread = threading.Thread(target=self.adjust_num_workers)\n",
    "        self.adjust_thread.start()\n",
    "\n",
    "    def stop_adjusting(self):\n",
    "        self.adjusting = False\n",
    "        self.adjust_thread.join()\n",
    "\n",
    "    def get_loader(self):\n",
    "        return self.loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정\n",
    "batch_size = 64\n",
    "image_size = 320\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "nc = 3\n",
    "num_epochs = 50\n",
    "lr = 0.0001\n",
    "beta1 = 0.5\n",
    "ngpu = 1\n",
    "\n",
    "\n",
    "base_dir = './cat_faces/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000})\n",
      "Splitting dataset into training and validation sets...\n",
      "Training and validation data are ready.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU usage: 12%\n",
      "Increasing num_workers to 5\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 및 증강\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),  # 이미지를 320x320으로 리사이즈\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(45),\n",
    "        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n",
    "        transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),  # 이미지를 320x320으로 리사이즈\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# 전체 데이터셋 로드\n",
    "full_dataset = datasets.ImageFolder(base_dir, transform=data_transforms['train'])\n",
    "\n",
    "# 클래스별 이미지 개수 출력\n",
    "class_counts = Counter([full_dataset.targets[i] for i in range(len(full_dataset))])\n",
    "print(\"Original class distribution:\", class_counts)\n",
    "\n",
    "print(\"Splitting dataset into training and validation sets...\")\n",
    "# 데이터셋을 훈련과 검증 세트로 나누기 (예: 80% 훈련, 20% 검증)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# 훈련 데이터셋과 검증 데이터셋 각각에 다른 변환 적용\n",
    "train_dataset.dataset.transform = data_transforms['train']\n",
    "val_dataset.dataset.transform = data_transforms['val']\n",
    "\n",
    "# DynamicDataLoader 사용\n",
    "dynamic_loader = DynamicDataLoader(train_dataset, batch_size=32, num_workers=4, pin_memory=True, prefetch_factor=4)\n",
    "dynamic_loader.start_adjusting()\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True, prefetch_factor=4, persistent_workers=True)\n",
    "\n",
    "dataloaders = {'train': dynamic_loader.get_loader(), 'val': val_loader}\n",
    "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "print(\"Training and validation data are ready.\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU usage: 11%\n",
      "Increasing num_workers to 5\n",
      "[0/50][0/141] Loss_D: 1.4593 Loss_G: 2.2316 D(x): 0.6020 D(G(z)): 0.6031 / 0.1117\n",
      "[0/50][50/141] Loss_D: 0.0322 Loss_G: 6.3756 D(x): 0.9790 D(G(z)): 0.0106 / 0.0018\n",
      "[0/50][100/141] Loss_D: 0.0115 Loss_G: 6.9742 D(x): 0.9930 D(G(z)): 0.0044 / 0.0010\n",
      "Current GPU usage: 0%\n",
      "Increasing num_workers to 6\n",
      "[1/50][0/141] Loss_D: 0.0086 Loss_G: 7.3667 D(x): 0.9942 D(G(z)): 0.0027 / 0.0007\n",
      "[1/50][50/141] Loss_D: 0.0057 Loss_G: 7.4138 D(x): 0.9961 D(G(z)): 0.0018 / 0.0006\n",
      "[1/50][100/141] Loss_D: 0.0037 Loss_G: 7.4008 D(x): 0.9977 D(G(z)): 0.0013 / 0.0006\n",
      "[2/50][0/141] Loss_D: 0.0029 Loss_G: 7.5994 D(x): 0.9980 D(G(z)): 0.0009 / 0.0005\n",
      "Current GPU usage: 55%\n",
      "Increasing num_workers to 7\n",
      "[2/50][50/141] Loss_D: 0.0020 Loss_G: 7.6128 D(x): 0.9988 D(G(z)): 0.0008 / 0.0005\n",
      "[2/50][100/141] Loss_D: 0.0016 Loss_G: 7.8714 D(x): 0.9990 D(G(z)): 0.0007 / 0.0004\n",
      "[3/50][0/141] Loss_D: 0.0018 Loss_G: 7.7876 D(x): 0.9989 D(G(z)): 0.0007 / 0.0004\n",
      "Current GPU usage: 62%\n",
      "Increasing num_workers to 8\n",
      "[3/50][50/141] Loss_D: 0.0012 Loss_G: 8.0775 D(x): 0.9992 D(G(z)): 0.0005 / 0.0003\n",
      "[3/50][100/141] Loss_D: 0.0009 Loss_G: 8.3465 D(x): 0.9995 D(G(z)): 0.0003 / 0.0002\n",
      "[4/50][0/141] Loss_D: 0.0008 Loss_G: 8.3878 D(x): 0.9995 D(G(z)): 0.0003 / 0.0002\n",
      "Current GPU usage: 49%\n",
      "Increasing num_workers to 9\n",
      "[4/50][50/141] Loss_D: 0.0008 Loss_G: 8.4477 D(x): 0.9995 D(G(z)): 0.0003 / 0.0002\n",
      "[4/50][100/141] Loss_D: 0.0007 Loss_G: 8.4617 D(x): 0.9995 D(G(z)): 0.0003 / 0.0002\n",
      "[5/50][0/141] Loss_D: 0.0006 Loss_G: 8.5264 D(x): 0.9996 D(G(z)): 0.0003 / 0.0002\n",
      "Current GPU usage: 35%\n",
      "Increasing num_workers to 10\n",
      "[5/50][50/141] Loss_D: 0.0005 Loss_G: 8.6834 D(x): 0.9997 D(G(z)): 0.0002 / 0.0002\n",
      "[5/50][100/141] Loss_D: 0.0005 Loss_G: 8.6040 D(x): 0.9998 D(G(z)): 0.0003 / 0.0002\n",
      "[6/50][0/141] Loss_D: 0.0005 Loss_G: 8.7817 D(x): 0.9997 D(G(z)): 0.0002 / 0.0002\n",
      "Current GPU usage: 30%\n",
      "Increasing num_workers to 11\n",
      "[6/50][50/141] Loss_D: 0.0004 Loss_G: 8.7954 D(x): 0.9998 D(G(z)): 0.0002 / 0.0002\n",
      "[6/50][100/141] Loss_D: 0.0003 Loss_G: 9.0124 D(x): 0.9998 D(G(z)): 0.0001 / 0.0001\n",
      "[7/50][0/141] Loss_D: 0.0003 Loss_G: 9.0118 D(x): 0.9998 D(G(z)): 0.0002 / 0.0001\n",
      "Current GPU usage: 47%\n",
      "Increasing num_workers to 12\n",
      "[7/50][50/141] Loss_D: 0.0003 Loss_G: 9.3713 D(x): 0.9998 D(G(z)): 0.0001 / 0.0001\n",
      "[7/50][100/141] Loss_D: 0.0002 Loss_G: 9.6531 D(x): 0.9998 D(G(z)): 0.0001 / 0.0001\n",
      "[8/50][0/141] Loss_D: 0.0003 Loss_G: 9.6692 D(x): 0.9998 D(G(z)): 0.0001 / 0.0001\n",
      "Current GPU usage: 31%\n",
      "Increasing num_workers to 13\n",
      "[8/50][50/141] Loss_D: 0.0002 Loss_G: 9.4384 D(x): 0.9999 D(G(z)): 0.0001 / 0.0001\n",
      "[8/50][100/141] Loss_D: 0.0002 Loss_G: 9.5090 D(x): 0.9999 D(G(z)): 0.0001 / 0.0001\n",
      "[9/50][0/141] Loss_D: 0.0002 Loss_G: 9.7291 D(x): 0.9998 D(G(z)): 0.0001 / 0.0001\n",
      "Current GPU usage: 40%\n",
      "Increasing num_workers to 14\n",
      "[9/50][50/141] Loss_D: 0.0002 Loss_G: 9.7045 D(x): 0.9999 D(G(z)): 0.0001 / 0.0001\n",
      "[9/50][100/141] Loss_D: 0.0002 Loss_G: 9.8563 D(x): 0.9998 D(G(z)): 0.0001 / 0.0001\n",
      "[10/50][0/141] Loss_D: 0.0002 Loss_G: 10.3397 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[10/50][50/141] Loss_D: 0.0002 Loss_G: 9.8343 D(x): 0.9999 D(G(z)): 0.0001 / 0.0001\n",
      "Current GPU usage: 51%\n",
      "Increasing num_workers to 15\n",
      "[10/50][100/141] Loss_D: 0.0001 Loss_G: 9.9040 D(x): 0.9999 D(G(z)): 0.0001 / 0.0001\n",
      "[11/50][0/141] Loss_D: 0.0001 Loss_G: 10.1836 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[11/50][50/141] Loss_D: 0.0001 Loss_G: 10.0349 D(x): 0.9999 D(G(z)): 0.0001 / 0.0000\n",
      "Current GPU usage: 43%\n",
      "Increasing num_workers to 16\n",
      "[11/50][100/141] Loss_D: 0.0001 Loss_G: 10.0397 D(x): 0.9999 D(G(z)): 0.0001 / 0.0000\n",
      "[12/50][0/141] Loss_D: 0.0001 Loss_G: 10.1746 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[12/50][50/141] Loss_D: 0.0001 Loss_G: 10.2591 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 46%\n",
      "[12/50][100/141] Loss_D: 0.0001 Loss_G: 10.5705 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[13/50][0/141] Loss_D: 0.0001 Loss_G: 10.5553 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[13/50][50/141] Loss_D: 0.0001 Loss_G: 10.2477 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 38%\n",
      "[13/50][100/141] Loss_D: 0.0001 Loss_G: 10.4750 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[14/50][0/141] Loss_D: 0.0001 Loss_G: 10.4020 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[14/50][50/141] Loss_D: 0.0001 Loss_G: 10.6447 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 33%\n",
      "[14/50][100/141] Loss_D: 0.0001 Loss_G: 10.6391 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[15/50][0/141] Loss_D: 0.0001 Loss_G: 10.5445 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[15/50][50/141] Loss_D: 0.0001 Loss_G: 10.7753 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 44%\n",
      "[15/50][100/141] Loss_D: 0.0000 Loss_G: 11.0596 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[16/50][0/141] Loss_D: 0.0001 Loss_G: 10.9821 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[16/50][50/141] Loss_D: 0.0000 Loss_G: 11.1089 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 36%\n",
      "[16/50][100/141] Loss_D: 0.0001 Loss_G: 11.2445 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[17/50][0/141] Loss_D: 0.0000 Loss_G: 11.0971 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[17/50][50/141] Loss_D: 0.0000 Loss_G: 11.2538 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 43%\n",
      "[17/50][100/141] Loss_D: 0.0001 Loss_G: 11.1449 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[18/50][0/141] Loss_D: 0.0000 Loss_G: 11.2146 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[18/50][50/141] Loss_D: 0.0000 Loss_G: 11.3511 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[18/50][100/141] Loss_D: 0.0000 Loss_G: 11.2349 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 42%\n",
      "[19/50][0/141] Loss_D: 0.0000 Loss_G: 11.1359 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[19/50][50/141] Loss_D: 0.0000 Loss_G: 11.0604 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[19/50][100/141] Loss_D: 0.0000 Loss_G: 11.3569 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 33%\n",
      "[20/50][0/141] Loss_D: 0.0001 Loss_G: 10.9693 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[20/50][50/141] Loss_D: 0.0000 Loss_G: 11.6783 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[20/50][100/141] Loss_D: 0.0000 Loss_G: 11.8591 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 43%\n",
      "[21/50][0/141] Loss_D: 0.0000 Loss_G: 11.7039 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[21/50][50/141] Loss_D: 0.0000 Loss_G: 11.7594 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[21/50][100/141] Loss_D: 0.0000 Loss_G: 11.7825 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 55%\n",
      "[22/50][0/141] Loss_D: 0.0000 Loss_G: 11.7733 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[22/50][50/141] Loss_D: 0.0000 Loss_G: 11.9648 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[22/50][100/141] Loss_D: 0.0000 Loss_G: 12.0262 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 61%\n",
      "[23/50][0/141] Loss_D: 0.0000 Loss_G: 11.9563 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[23/50][50/141] Loss_D: 0.0000 Loss_G: 11.7692 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[23/50][100/141] Loss_D: 0.0000 Loss_G: 11.9153 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 52%\n",
      "[24/50][0/141] Loss_D: 0.0000 Loss_G: 11.9526 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[24/50][50/141] Loss_D: 0.0000 Loss_G: 11.8591 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[24/50][100/141] Loss_D: 0.0000 Loss_G: 11.8418 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 37%\n",
      "[25/50][0/141] Loss_D: 0.0000 Loss_G: 11.8519 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[25/50][50/141] Loss_D: 0.0000 Loss_G: 12.0362 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[25/50][100/141] Loss_D: 0.0000 Loss_G: 12.2998 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 34%\n",
      "[26/50][0/141] Loss_D: 0.0000 Loss_G: 12.3986 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[26/50][50/141] Loss_D: 0.0000 Loss_G: 12.3552 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[26/50][100/141] Loss_D: 0.0000 Loss_G: 12.3652 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[27/50][0/141] Loss_D: 0.0000 Loss_G: 12.3583 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 55%\n",
      "[27/50][50/141] Loss_D: 0.0000 Loss_G: 12.4313 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[27/50][100/141] Loss_D: 0.0000 Loss_G: 12.4377 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[28/50][0/141] Loss_D: 0.0000 Loss_G: 12.3850 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 54%\n",
      "[28/50][50/141] Loss_D: 0.0000 Loss_G: 12.4128 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[28/50][100/141] Loss_D: 0.0000 Loss_G: 12.2347 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[29/50][0/141] Loss_D: 0.0000 Loss_G: 12.3368 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 56%\n",
      "[29/50][50/141] Loss_D: 0.0000 Loss_G: 12.3999 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[29/50][100/141] Loss_D: 0.0000 Loss_G: 12.6025 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[30/50][0/141] Loss_D: 0.0000 Loss_G: 12.7769 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 34%\n",
      "[30/50][50/141] Loss_D: 0.0000 Loss_G: 12.9205 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[30/50][100/141] Loss_D: 0.0000 Loss_G: 12.8207 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[31/50][0/141] Loss_D: 0.0000 Loss_G: 13.0515 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[31/50][50/141] Loss_D: 0.0000 Loss_G: 13.1636 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 49%\n",
      "[31/50][100/141] Loss_D: 0.0000 Loss_G: 13.2682 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[32/50][0/141] Loss_D: 0.0000 Loss_G: 13.2408 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[32/50][50/141] Loss_D: 0.0000 Loss_G: 13.4390 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 52%\n",
      "[32/50][100/141] Loss_D: 0.0000 Loss_G: 13.5210 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[33/50][0/141] Loss_D: 0.0000 Loss_G: 13.6405 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[33/50][50/141] Loss_D: 0.0000 Loss_G: 13.5153 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 35%\n",
      "[33/50][100/141] Loss_D: 0.0000 Loss_G: 13.4563 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[34/50][0/141] Loss_D: 0.0000 Loss_G: 13.6072 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[34/50][50/141] Loss_D: 0.0000 Loss_G: 13.5456 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 54%\n",
      "[34/50][100/141] Loss_D: 0.0000 Loss_G: 13.3712 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[35/50][0/141] Loss_D: 0.0000 Loss_G: 13.3999 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[35/50][50/141] Loss_D: 0.0000 Loss_G: 13.3210 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 46%\n",
      "[35/50][100/141] Loss_D: 0.0000 Loss_G: 13.3348 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[36/50][0/141] Loss_D: 0.0000 Loss_G: 13.3271 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[36/50][50/141] Loss_D: 0.0000 Loss_G: 13.2193 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[36/50][100/141] Loss_D: 0.0000 Loss_G: 13.2215 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 35%\n",
      "[37/50][0/141] Loss_D: 0.0000 Loss_G: 13.1956 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[37/50][50/141] Loss_D: 0.0000 Loss_G: 13.1101 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[37/50][100/141] Loss_D: 0.0000 Loss_G: 13.1839 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 43%\n",
      "[38/50][0/141] Loss_D: 0.0000 Loss_G: 13.1838 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[38/50][50/141] Loss_D: 0.0000 Loss_G: 13.3030 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[38/50][100/141] Loss_D: 0.0000 Loss_G: 13.3601 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 39%\n",
      "[39/50][0/141] Loss_D: 0.0000 Loss_G: 13.1902 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[39/50][50/141] Loss_D: 0.0000 Loss_G: 13.2968 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[39/50][100/141] Loss_D: 0.0000 Loss_G: 13.2714 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 41%\n",
      "[40/50][0/141] Loss_D: 0.0000 Loss_G: 13.5070 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[40/50][50/141] Loss_D: 0.0000 Loss_G: 13.3591 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[40/50][100/141] Loss_D: 0.0000 Loss_G: 13.5096 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[41/50][0/141] Loss_D: 0.0000 Loss_G: 13.5341 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 33%\n",
      "[41/50][50/141] Loss_D: 0.0000 Loss_G: 13.6481 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[41/50][100/141] Loss_D: 0.0000 Loss_G: 13.5783 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[42/50][0/141] Loss_D: 0.0000 Loss_G: 13.5839 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 32%\n",
      "[42/50][50/141] Loss_D: 0.0000 Loss_G: 13.6291 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[42/50][100/141] Loss_D: 0.0000 Loss_G: 13.6152 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[43/50][0/141] Loss_D: 0.0000 Loss_G: 13.7158 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 58%\n",
      "[43/50][50/141] Loss_D: 0.0000 Loss_G: 13.6452 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[43/50][100/141] Loss_D: 0.0000 Loss_G: 13.6457 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[44/50][0/141] Loss_D: 0.0000 Loss_G: 13.6442 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 47%\n",
      "[44/50][50/141] Loss_D: 0.0000 Loss_G: 13.5748 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[44/50][100/141] Loss_D: 0.0000 Loss_G: 13.5801 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[45/50][0/141] Loss_D: 0.0000 Loss_G: 13.4493 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 60%\n",
      "[45/50][50/141] Loss_D: 0.0000 Loss_G: 13.2226 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[45/50][100/141] Loss_D: 0.0000 Loss_G: 13.0780 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[46/50][0/141] Loss_D: 0.0000 Loss_G: 13.2498 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[46/50][50/141] Loss_D: 0.0000 Loss_G: 13.3919 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 54%\n",
      "[46/50][100/141] Loss_D: 0.0000 Loss_G: 13.5490 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[47/50][0/141] Loss_D: 0.0000 Loss_G: 13.5519 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[47/50][50/141] Loss_D: 0.0000 Loss_G: 13.8207 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 50%\n",
      "[47/50][100/141] Loss_D: 0.0000 Loss_G: 13.8490 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[48/50][0/141] Loss_D: 0.0000 Loss_G: 14.0209 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[48/50][50/141] Loss_D: 0.0000 Loss_G: 14.0161 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 55%\n",
      "[48/50][100/141] Loss_D: 0.0000 Loss_G: 14.0492 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[49/50][0/141] Loss_D: 0.0000 Loss_G: 14.0980 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[49/50][50/141] Loss_D: 0.0000 Loss_G: 14.2758 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "Current GPU usage: 47%\n",
      "[49/50][100/141] Loss_D: 0.0000 Loss_G: 14.2593 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 이 셀은 15분정도 시간이 걸리므로 이미 파일이 있거나 실행한적이 있으면 PASS\n",
    "\n",
    "# DCGAN 학습\n",
    "dataset = datasets.ImageFolder(root=base_dir,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize((image_size, image_size)),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 모델 초기화\n",
    "netG = Generator(nz, ngf, nc).to(device)\n",
    "netD = Discriminator(nc, ndf, image_size).to(device)\n",
    "\n",
    "# 손실 함수와 최적화 설정\n",
    "criterion = nn.BCELoss()\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        # 실제 이미지로 Discriminator 학습\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size, 1, 1, 1), real_label, dtype=torch.float, device=device)  # label 크기 조정\n",
    "        output = netD(real_cpu)\n",
    "        errD_real = criterion(output, label.expand_as(output))\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # 가짜 이미지로 Discriminator 학습\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, label.expand_as(output))\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Generator 학습\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, label.expand_as(output))\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f'[{epoch}/{num_epochs}][{i}/{len(dataloader)}] '\n",
    "                  f'Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f} '\n",
    "                  f'D(x): {D_x:.4f} D(G(z)): {D_G_z1:.4f} / {D_G_z2:.4f}')\n",
    "\n",
    "    # 학습 진행 상황 시각화\n",
    "    if epoch % 10 == 0:\n",
    "        vutils.save_image(real_cpu, f'real_samples_epoch_{epoch}.png', normalize=True)\n",
    "        fake = netG(fixed_noise)\n",
    "        vutils.save_image(fake.detach(), f'fake_samples_epoch_{epoch}.png', normalize=True)\n",
    "\n",
    "# 최종 Generator 저장\n",
    "torch.save(netG.state_dict(), 'dcgan_generator.pth')\n",
    "\n",
    "# 생성된 이미지 저장 경로 설정\n",
    "gen_images_dir = './generated_images/'\n",
    "os.makedirs(gen_images_dir, exist_ok=True)\n",
    "\n",
    "# 학습된 Generator 로드\n",
    "netG.load_state_dict(torch.load('dcgan_generator.pth'))\n",
    "netG.eval()\n",
    "\n",
    "# 각 품종에 대한 새로운 이미지 생성 및 저장\n",
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(gen_images_dir, class_name)\n",
    "    os.makedirs(class_dir, exist_ok=True)\n",
    "    for i in range(100):  # 각 품종당 100장 생성\n",
    "        noise = torch.randn(1, nz, 1, 1, device=device)\n",
    "        with torch.no_grad():\n",
    "            fake = netG(noise).detach().cpu()\n",
    "        fake_image = (fake[0] * 0.5 + 0.5) * 255\n",
    "        fake_image = fake_image.permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "        img = Image.fromarray(fake_image)\n",
    "        img.save(os.path.join(class_dir, f'fake_{i}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU usage: 18%\n",
      "Increasing num_workers to 5\n"
     ]
    }
   ],
   "source": [
    "gen_images_dir = './generated_images/'\n",
    "\n",
    "# 기존 데이터셋에 생성된 이미지 추가\n",
    "new_data_transforms = transforms.Compose([\n",
    "    transforms.Resize(360),  \n",
    "    transforms.CenterCrop(320),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "generated_dataset = datasets.ImageFolder(root=gen_images_dir, transform=new_data_transforms)\n",
    "full_dataset = ConcatDataset([full_dataset, generated_dataset])\n",
    "\n",
    "# 기존 데이터셋을 다시 분할\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataset.dataset.transform = data_transforms['train']\n",
    "val_dataset.dataset.transform = data_transforms['val']\n",
    "\n",
    "dynamic_loader = DynamicDataLoader(train_dataset, batch_size=32, num_workers=4, pin_memory=True, prefetch_factor=4)\n",
    "dynamic_loader.start_adjusting()\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True, prefetch_factor=4, persistent_workers=True)\n",
    "\n",
    "dataloaders = {'train': dynamic_loader.get_loader(), 'val': val_loader}\n",
    "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
    "class_names = full_dataset.datasets[0].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output features: 204800\n"
     ]
    }
   ],
   "source": [
    "# ResNeSt-200 모델 로드\n",
    "base_model = timm.create_model('resnest200e', pretrained=True).to(device)\n",
    "\n",
    "# 모델의 출력 크기를 확인\n",
    "dummy_input = torch.randn(1, 3, 320, 320).to(device)  # 이미지 크기 320x320으로 변경\n",
    "base_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    dummy_output = base_model.forward_features(dummy_input)\n",
    "    num_features = dummy_output.shape[1] * dummy_output.shape[2] * dummy_output.shape[3]\n",
    "    print(f'Output features: {num_features}')\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, base_model, num_classes):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc = nn.Linear(num_features, num_classes)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        x = self.base_model.forward_features(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = CustomModel(base_model, len(class_names)).to(device)\n",
    "\n",
    "# 모든 레이어의 requires_grad를 True로 설정하여 고정 해제\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "scaler = amp.GradScaler()\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(dataloaders['train']), epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "Current GPU usage: 80%\n",
      "Increasing num_workers to 6\n",
      "Current GPU usage: 84%\n",
      "Increasing num_workers to 6\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 89%\n",
      "Current GPU usage: 93%\n",
      "Current GPU usage: 90%\n",
      "train Loss: 2.1008 Acc: 0.3114\n",
      "Current GPU usage: 1%\n",
      "Increasing num_workers to 7\n",
      "val Loss: 2.4150 Acc: 0.3808\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kimsu\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU usage: 79%\n",
      "Increasing num_workers to 7\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 90%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 69%\n",
      "Increasing num_workers to 8\n",
      "Current GPU usage: 87%\n",
      "train Loss: 1.7573 Acc: 0.3889\n",
      "Current GPU usage: 90%\n",
      "val Loss: 1.6705 Acc: 0.3980\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 93%\n",
      "train Loss: 1.5010 Acc: 0.4554\n",
      "val Loss: 1.5029 Acc: 0.4672\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "Current GPU usage: 84%\n",
      "Increasing num_workers to 9\n",
      "Current GPU usage: 89%\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 71%\n",
      "Increasing num_workers to 10\n",
      "Current GPU usage: 92%\n",
      "train Loss: 1.3283 Acc: 0.5172\n",
      "val Loss: 3.7161 Acc: 0.3742\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "Current GPU usage: 55%\n",
      "Increasing num_workers to 11\n",
      "Current GPU usage: 78%\n",
      "Increasing num_workers to 8\n",
      "Current GPU usage: 89%\n",
      "Current GPU usage: 90%\n",
      "Current GPU usage: 88%\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 77%\n",
      "Increasing num_workers to 12\n",
      "train Loss: 1.2426 Acc: 0.5432\n",
      "Current GPU usage: 90%\n",
      "val Loss: 1.4354 Acc: 0.4838\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "Current GPU usage: 93%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 90%\n",
      "train Loss: 1.2433 Acc: 0.5418\n",
      "val Loss: 1.6084 Acc: 0.4955\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 87%\n",
      "Current GPU usage: 80%\n",
      "Increasing num_workers to 9\n",
      "Current GPU usage: 76%\n",
      "Increasing num_workers to 13\n",
      "Current GPU usage: 73%\n",
      "Increasing num_workers to 10\n",
      "Current GPU usage: 91%\n",
      "train Loss: 1.1206 Acc: 0.5838\n",
      "Current GPU usage: 87%\n",
      "val Loss: 1.9077 Acc: 0.5030\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 88%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 88%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 92%\n",
      "train Loss: 1.0376 Acc: 0.6282\n",
      "val Loss: 1.4226 Acc: 0.5131\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 93%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 93%\n",
      "Current GPU usage: 92%\n",
      "train Loss: 0.9166 Acc: 0.6576\n",
      "Current GPU usage: 89%\n",
      "val Loss: 1.4737 Acc: 0.5126\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 88%\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 93%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 85%\n",
      "train Loss: 0.8003 Acc: 0.7074\n",
      "val Loss: 2.0500 Acc: 0.4970\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 91%\n",
      "train Loss: 0.7715 Acc: 0.7201\n",
      "Current GPU usage: 90%\n",
      "val Loss: 1.7429 Acc: 0.4934\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 90%\n",
      "Current GPU usage: 93%\n",
      "Current GPU usage: 92%\n",
      "train Loss: 0.7627 Acc: 0.7249\n",
      "Current GPU usage: 72%\n",
      "Increasing num_workers to 11\n",
      "val Loss: 1.6992 Acc: 0.4904\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "Current GPU usage: 90%\n",
      "Current GPU usage: 90%\n",
      "Current GPU usage: 88%\n",
      "Current GPU usage: 71%\n",
      "Increasing num_workers to 12\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 89%\n",
      "train Loss: 0.7294 Acc: 0.7365\n",
      "val Loss: 1.7265 Acc: 0.4894\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 77%\n",
      "Increasing num_workers to 13\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 91%\n",
      "train Loss: 0.7521 Acc: 0.7247\n",
      "Current GPU usage: 91%\n",
      "val Loss: 1.8532 Acc: 0.4955\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 77%\n",
      "Increasing num_workers to 14\n",
      "Current GPU usage: 90%\n",
      "Current GPU usage: 90%\n",
      "Current GPU usage: 78%\n",
      "Increasing num_workers to 14\n",
      "train Loss: 0.6505 Acc: 0.7582\n",
      "val Loss: 1.7978 Acc: 0.5167\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "Current GPU usage: 90%\n",
      "Current GPU usage: 90%\n",
      "Current GPU usage: 87%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 90%\n",
      "Current GPU usage: 81%\n",
      "Increasing num_workers to 15\n",
      "train Loss: 0.5069 Acc: 0.8105\n",
      "val Loss: 1.9413 Acc: 0.5005\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "Current GPU usage: 81%\n",
      "Increasing num_workers to 15\n",
      "Current GPU usage: 89%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 86%\n",
      "Current GPU usage: 84%\n",
      "Increasing num_workers to 16\n",
      "Current GPU usage: 93%\n",
      "Current GPU usage: 92%\n",
      "train Loss: 0.4702 Acc: 0.8226\n",
      "Current GPU usage: 90%\n",
      "val Loss: 1.9493 Acc: 0.5056\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 84%\n",
      "Current GPU usage: 90%\n",
      "Current GPU usage: 90%\n",
      "Current GPU usage: 69%\n",
      "Increasing num_workers to 16\n",
      "train Loss: 0.4190 Acc: 0.8455\n",
      "val Loss: 3.0612 Acc: 0.4914\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "Current GPU usage: 84%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 89%\n",
      "Current GPU usage: 76%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 92%\n",
      "train Loss: 0.4298 Acc: 0.8388\n",
      "Current GPU usage: 89%\n",
      "val Loss: 2.1323 Acc: 0.5071\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "Current GPU usage: 79%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 89%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 90%\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 93%\n",
      "train Loss: 0.4112 Acc: 0.8484\n",
      "val Loss: 2.2893 Acc: 0.5116\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 90%\n",
      "Current GPU usage: 90%\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 91%\n",
      "train Loss: 0.3827 Acc: 0.8563\n",
      "val Loss: 2.4360 Acc: 0.5081\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 87%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 93%\n",
      "Current GPU usage: 91%\n",
      "Current GPU usage: 92%\n",
      "train Loss: 0.4523 Acc: 0.8350\n",
      "Current GPU usage: 91%\n",
      "val Loss: 2.2448 Acc: 0.5141\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "Current GPU usage: 92%\n",
      "Current GPU usage: 93%\n",
      "Current GPU usage: 87%\n",
      "Current GPU usage: 86%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[0;32m     37\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 모델 학습 및 저장\n",
    "num_epochs = 50\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "patience = 8  # 조기 종료를 위한 patience 설정\n",
    "trigger_times = 0  # 조기 종료를 위한 트리거 시간 초기화\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                with amp.autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        if phase == 'val' and epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            trigger_times = 0  # 조기 종료 트리거 초기화\n",
    "            \n",
    "        elif phase == 'val':\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping!')\n",
    "                model.load_state_dict(best_model_wts)\n",
    "                dynamic_loader.stop_adjusting()  # 동적 조정 멈춤\n",
    "                exit()  # 학습 종료\n",
    "        \n",
    "        if phase == 'val':\n",
    "            val_loss = epoch_loss  # validation 손실 저장\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    print()\n",
    "\n",
    "print('Training complete')\n",
    "print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "model.load_state_dict(best_model_wts)\n",
    "torch.save(model.state_dict(), 'cat_breeds_resnest200e.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU usage: 1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU usage: 0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU usage: 1%\n"
     ]
    }
   ],
   "source": [
    "# 동적 조정 멈춤\n",
    "dynamic_loader.stop_adjusting()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
