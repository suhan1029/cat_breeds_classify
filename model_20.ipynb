{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU usage: 0%\n",
      "Increasing num_workers to 5\n",
      "Current GPU usage: 0%\n",
      "Increasing num_workers to 7\n",
      "Current GPU usage: 8%\n",
      "Increasing num_workers to 6\n",
      "Current GPU usage: 9%\n",
      "Increasing num_workers to 8\n",
      "Current GPU usage: 10%\n",
      "Increasing num_workers to 7\n",
      "Current GPU usage: 4%\n",
      "Increasing num_workers to 9\n",
      "Current GPU usage: 7%\n",
      "Increasing num_workers to 8\n",
      "Current GPU usage: 5%\n",
      "Increasing num_workers to 10\n",
      "Current GPU usage: 6%\n",
      "Increasing num_workers to 9\n",
      "Current GPU usage: 8%\n",
      "Increasing num_workers to 11\n",
      "Current GPU usage: 10%\n",
      "Increasing num_workers to 10\n",
      "Current GPU usage: 21%\n",
      "Increasing num_workers to 12\n",
      "Current GPU usage: 9%\n",
      "Increasing num_workers to 11\n",
      "Current GPU usage: 3%\n",
      "Increasing num_workers to 13\n",
      "Current GPU usage: 0%\n",
      "Increasing num_workers to 12\n",
      "Current GPU usage: 8%\n",
      "Increasing num_workers to 14\n",
      "Current GPU usage: 8%\n",
      "Increasing num_workers to 13\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import copy\n",
    "import torch.cuda.amp as amp\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Define custom dataset\n",
    "class CatBreedsDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# GPU usage monitoring function\n",
    "def get_gpu_usage():\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], \n",
    "                            stdout=subprocess.PIPE)\n",
    "    return int(result.stdout.decode('utf-8').strip())\n",
    "\n",
    "# Dynamic DataLoader class\n",
    "class DynamicDataLoader:\n",
    "    def __init__(self, dataset, batch_size=32, num_workers=4, pin_memory=True, prefetch_factor=2):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.prefetch_factor = prefetch_factor\n",
    "        self.loader = self.create_loader()\n",
    "        self.adjusting = False\n",
    "        self.target_gpu_usage = 95\n",
    "\n",
    "    def create_loader(self):\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, \n",
    "                          pin_memory=self.pin_memory, prefetch_factor=self.prefetch_factor, persistent_workers=True)\n",
    "\n",
    "    def adjust_num_workers(self):\n",
    "        while self.adjusting:\n",
    "            gpu_usage = get_gpu_usage()\n",
    "            print(f\"Current GPU usage: {gpu_usage}%\")\n",
    "            if (gpu_usage < self.target_gpu_usage - 10) and (self.num_workers < 16):\n",
    "                self.num_workers += 1\n",
    "                print(f\"Increasing num_workers to {self.num_workers}\")\n",
    "            elif (gpu_usage > self.target_gpu_usage + 10) and (self.num_workers > 1):\n",
    "                self.num_workers -= 1\n",
    "                print(f\"Decreasing num_workers to {self.num_workers}\")\n",
    "            self.loader = self.create_loader()\n",
    "            time.sleep(20)\n",
    "\n",
    "    def start_adjusting(self):\n",
    "        self.adjusting = True\n",
    "        self.adjust_thread = threading.Thread(target=self.adjust_num_workers)\n",
    "        self.adjust_thread.start()\n",
    "\n",
    "    def stop_adjusting(self):\n",
    "        self.adjusting = False\n",
    "        self.adjust_thread.join()\n",
    "\n",
    "    def get_loader(self):\n",
    "        return self.loader\n",
    "\n",
    "# Check if GPU is available\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA is not available. Please check your GPU installation.\")\n",
    "\n",
    "# Set the path to your dataset\n",
    "data_dir = './CatBreeds/'\n",
    "categories = os.listdir(data_dir)\n",
    "\n",
    "# Prepare data\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for idx, category in enumerate(categories):\n",
    "    category_path = os.path.join(data_dir, category)\n",
    "    for img_name in os.listdir(category_path):\n",
    "        image_paths.append(os.path.join(category_path, img_name))\n",
    "        labels.append(idx)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(image_paths, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n",
    "# Define transformations\n",
    "size = 320\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((size, size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "    transforms.RandomResizedCrop(size, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((size, size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CatBreedsDataset(train_paths, train_labels, transform=train_transforms)\n",
    "val_dataset = CatBreedsDataset(val_paths, val_labels, transform=val_transforms)\n",
    "\n",
    "# Create dynamic data loader\n",
    "dynamic_loader = DynamicDataLoader(train_dataset)\n",
    "dynamic_loader.start_adjusting()\n",
    "\n",
    "# Define the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_model = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.IMAGENET1K_V1).to(device)\n",
    "\n",
    "# Get the number of features\n",
    "dummy_input = torch.randn(1, 3, size, size).to(device)\n",
    "base_model.eval()\n",
    "with torch.no_grad():\n",
    "    dummy_output = base_model.features(dummy_input)\n",
    "    num_features = dummy_output.shape[1] * dummy_output.shape[2] * dummy_output.shape[3]\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, base_model, num_classes, dropout):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(num_features, num_classes)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "weight_decay = 0.001\n",
    "dropout = 0.5\n",
    "num_epochs = 10\n",
    "\n",
    "# Set up data loaders\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, prefetch_factor=4, persistent_workers=True)\n",
    "dataloaders = {'train': dynamic_loader.get_loader(), 'val': val_loader}\n",
    "\n",
    "# Model, criterion, optimizer, and scheduler\n",
    "model = CustomModel(base_model, len(categories), dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scaler = amp.GradScaler()\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(dataloaders['train']), epochs=num_epochs)\n",
    "\n",
    "# Training the model\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "patience = 12\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                with amp.autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "        epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        if phase == 'val' and epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            trigger_times = 0\n",
    "        elif phase == 'val':\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping!')\n",
    "                model.load_state_dict(best_model_wts)\n",
    "                dynamic_loader.stop_adjusting()\n",
    "                torch.save(model.state_dict(), 'cat_breed_classifier_best.pth')\n",
    "                exit()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "model.load_state_dict(best_model_wts)\n",
    "torch.save(model.state_dict(), 'cat_breed_classifier.pth')\n",
    "\n",
    "dynamic_loader.stop_adjusting()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
